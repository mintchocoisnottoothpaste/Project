{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105fd01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import adabound\n",
    "from pycox.evaluation.concordance import concordance_td\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d77d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAdam(optim.Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        \n",
    "        self.degenerated_to_sgd = degenerated_to_sgd\n",
    "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
    "            for param in params:\n",
    "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
    "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = group['buffer'][int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    elif self.degenerated_to_sgd:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = -1\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "                elif step_size > 0:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.read_csv('D:/Cho Lab Dropbox/연구과제별정리/02 목적과제_SPPEC_암종별(임상)_상희회준/위암/03 머신러닝_회준종혁다혜/00 ML_Data/DL_data_220629.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mydata = mydata[mydata['Age']>74]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f8f35",
   "metadata": {},
   "source": [
    "Redefine event indicator (Complication yes or no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a0e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = mydata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb8bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['complication']=np.where(mydata['Clavien_Dindo']> 1, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data = dat['complication']\n",
    "#event_data = mydata['Clavien_Dindo']\n",
    "#pd.crosstab(mydata['OP_year'], mydata['Clavien_Dindo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c473947",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_op_dat = dat[['Sex', \n",
    "        'Age',\n",
    "        'ASA_Score',\n",
    "        'Smoking',\n",
    "        'Drinking',\n",
    "        'BMI',\n",
    "        'Hypertension',\n",
    "        'Diabetes',\n",
    "        'Ass_condition_grp',\n",
    "        'Ass_lesion',\n",
    "        'Clinical_Stage_grp ',\n",
    "        'Histology',\n",
    "        'Lauren',\n",
    "        'Reconstruction',\n",
    "        'Combined_Resection',\n",
    "        'Platelets',\n",
    "        'Albumin',\n",
    "        'Cell_Count',\n",
    "        'Hemoglobin_status',\n",
    "        'Neutrophil_count_status'\n",
    "]]\n",
    "\n",
    "post_op_dat = mydata[['fStage_grp',\n",
    "                'Location',\n",
    "                'LN_Dissection',\n",
    "                'Operation',\n",
    "                'Intraop_cc',\n",
    "                'OP_time',\n",
    "                'Z_EBL'\n",
    "               ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed46f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsubject = dat.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5765c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "contvar = ['Age', \"Platelets\", \"Albumin\", \"Cell_Count\"]\n",
    "catevar= pre_op_dat.columns[[x not in contvar for x in pre_op_dat.columns]]\n",
    "\n",
    "contvar_post = ['OP_time', 'Z_EBL']\n",
    "catevar_post = post_op_dat.columns[[x not in contvar_post for x in post_op_dat.columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658385c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_op_dat2 = pre_op_dat.copy()\n",
    "pre_op_dat2[catevar] = pre_op_dat2[catevar].astype(\"category\")\n",
    "\n",
    "post_op_dat2 = post_op_dat.copy()\n",
    "post_op_dat2[catevar_post] = post_op_dat2[catevar_post].astype(\"category\")\n",
    "\n",
    "print(pre_op_dat2.dtypes, post_op_dat2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fcb20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_op_dat3 = pd.get_dummies(pre_op_dat2, columns=catevar)\n",
    "pre_dat = pre_op_dat3.to_numpy()\n",
    "\n",
    "post_op_dat3 = pd.get_dummies(post_op_dat2, columns=catevar_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c44fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_op_dat3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43eebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_op_dat3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_col = ['Z_EBL']\n",
    "#Technique 1: Using mean to impute the missing values\n",
    "for i in missing_col:\n",
    "    post_op_dat3.loc[post_op_dat3.loc[:,i].isnull(),i]=post_op_dat3.loc[:,i].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a98b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_dat = post_op_dat3.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976037de",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dat = np.concatenate([pre_dat, post_dat], 1)\n",
    "print(pre_dat.shape, post_dat.shape, total_dat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OP_year = mydata['OP_year']\n",
    "testindex = np.isin(OP_year, [2015, 2017, 2019, 2021])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train = total_dat[np.invert(testindex)].copy()\n",
    "event_train = event_data.values[np.invert(testindex)].copy()\n",
    "\n",
    "total_test = total_dat[testindex].copy()\n",
    "event_test = event_data.values[testindex].copy()\n",
    "\n",
    "ntrain = total_train.shape[0]\n",
    "nsubject, ntrain, nsubject-ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b39b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_event = 1\n",
    "batch_size = 256\n",
    "hidden_size = 256\n",
    "\n",
    "train_data = []\n",
    "for i in range(ntrain):\n",
    "    train_data.append([total_train[i], event_train[i]])\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da314277",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available() # False\n",
    "if cuda:\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=128, num_layer=1, num_event=1):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.num_event = num_event\n",
    "        \n",
    "        self.hidden_in = nn.Linear(self.input_size, self.hidden_size)\n",
    "        hiddens = [\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        ]\n",
    "        self.hiddens = nn.Sequential(*((num_layer-1)*hiddens))\n",
    "        self.hidden_out = nn.Linear(self.hidden_size, 1)\n",
    "        self.activation = nn.LeakyReLU(inplace=True)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        ## x: (batch, input_size)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.hidden_in(x)\n",
    "        x = self.hiddens(self.activation(x))\n",
    "        x = self.hidden_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b66519",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size=41, hidden_size=128, num_layer=2, num_event=num_event).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c334c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for weight_decay in [1e-3]:\n",
    "    for hidden_size in [16, 32, 64, 128, 256]:\n",
    "        for num_layer in [1, 2, 3]:\n",
    "            \n",
    "            path = 'D:/models/prepost_binary/MultiMLP_{}hiddensize_{}layers_{:.0e}'.format(hidden_size, num_layer, weight_decay)\n",
    "            #if os.path.isfile(path):\n",
    "            #    continue\n",
    "            print(path[9:])\n",
    "\n",
    "            model = MLP(input_size=total_train.shape[-1], hidden_size=hidden_size, num_layer=num_layer, num_event=num_event).to(device)\n",
    "            #if os.path.isfile(path):\n",
    "            #    model.load_state_dict(torch.load(path, map_location = device))\n",
    "\n",
    "            lr = 1e-3\n",
    "            optimizer = adabound.AdaBound(model.parameters(), lr=lr, weight_decay=0)\n",
    "\n",
    "            loss_array = []\n",
    "            patience = 0\n",
    "            min_loss = np.inf\n",
    "            for e in range(int(1e6)):\n",
    "\n",
    "                loss_array_tmp = []\n",
    "\n",
    "                for total_batch, event_batch in train_loader:\n",
    "\n",
    "                    total_batch = total_batch.float()\n",
    "                    event_batch = event_batch.reshape(-1,1).float()\n",
    "\n",
    "                    y_pred = model(total_batch.to(device))\n",
    "\n",
    "                    norm = 0.\n",
    "                    for parameter in model.parameters():\n",
    "                        norm += torch.norm(parameter, p=1)\n",
    "\n",
    "                    loss1 = criterion(y_pred, event_batch.to(device))\n",
    "\n",
    "                    loss = loss1 + weight_decay*norm\n",
    "                    loss_array_tmp.append(loss1.item())\n",
    "\n",
    "                    model.zero_grad()\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "\n",
    "                loss_array.append(np.mean(loss_array_tmp))\n",
    "                if e % 100 == 0:\n",
    "                    print('Epoch: ' + str(e) + \n",
    "                          ', Loss: '+ f'{loss_array[-1]:.4e}')\n",
    "                if min_loss > loss_array[-1]:\n",
    "                    patience = 0\n",
    "                    min_loss = loss_array[-1]\n",
    "                    torch.save(model.state_dict(), path)\n",
    "                else:\n",
    "                    patience += 1\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                if patience > 1000:\n",
    "                    break\n",
    "\n",
    "            plt.plot(loss_array, label='Loss')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.yscale('log')\n",
    "            plt.title(path[2:])\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            total_train_sort = torch.FloatTensor(total_train)\n",
    "            total_test_sort = torch.FloatTensor(total_test)\n",
    "\n",
    "            model.load_state_dict(torch.load(path, map_location = device))\n",
    "\n",
    "            y_train = torch.sigmoid(model(total_train_sort.to(device))).detach().cpu().numpy()\n",
    "            y_test = torch.sigmoid(model(total_test_sort.to(device))).detach().cpu().numpy()\n",
    "\n",
    "            out_pred = np.where(y_train >= 0.5, 1, 0)\n",
    "            acc_train = accuracy_score(event_train, out_pred.flatten())\n",
    "            auc_train = roc_auc_score(event_train, y_train.flatten())\n",
    "            \n",
    "            out_pred = np.where(y_test >= 0.5, 1, 0)\n",
    "            acc_test = accuracy_score(event_test, out_pred.flatten())\n",
    "            auc_test = roc_auc_score(event_test, y_test.flatten())\n",
    "            print('-------------------------------------------------------')\n",
    "            print(path[9:])\n",
    "            print('Train accuracy = {:.4f}, Test accuracy = {:.4f}'.format(acc_train, acc_test))\n",
    "            print('Train AUC = {:.4f}, Test AUC = {:.4f}'.format(auc_train, auc_test))\n",
    "            print('=======================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9356c0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
